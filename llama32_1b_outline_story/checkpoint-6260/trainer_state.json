{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 20.0,
  "eval_steps": 500,
  "global_step": 6260,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.16,
      "grad_norm": 0.5461562871932983,
      "learning_rate": 9.8e-05,
      "loss": 3.1599,
      "step": 50
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.5148217082023621,
      "learning_rate": 0.00019800000000000002,
      "loss": 2.5368,
      "step": 100
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.5491719841957092,
      "learning_rate": 0.00019840909090909091,
      "loss": 2.4624,
      "step": 150
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.46424904465675354,
      "learning_rate": 0.00019678571428571428,
      "loss": 2.4425,
      "step": 200
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.435744047164917,
      "learning_rate": 0.00019516233766233768,
      "loss": 2.4208,
      "step": 250
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.46288901567459106,
      "learning_rate": 0.00019353896103896105,
      "loss": 2.454,
      "step": 300
    },
    {
      "epoch": 1.1184,
      "grad_norm": 0.45786619186401367,
      "learning_rate": 0.00019191558441558442,
      "loss": 2.4143,
      "step": 350
    },
    {
      "epoch": 1.2784,
      "grad_norm": 0.455468088388443,
      "learning_rate": 0.0001902922077922078,
      "loss": 2.4169,
      "step": 400
    },
    {
      "epoch": 1.4384000000000001,
      "grad_norm": 0.5080470442771912,
      "learning_rate": 0.00018866883116883118,
      "loss": 2.4266,
      "step": 450
    },
    {
      "epoch": 1.5984,
      "grad_norm": 0.5340607762336731,
      "learning_rate": 0.00018704545454545455,
      "loss": 2.3619,
      "step": 500
    },
    {
      "epoch": 1.7584,
      "grad_norm": 0.49404221773147583,
      "learning_rate": 0.00018542207792207794,
      "loss": 2.3958,
      "step": 550
    },
    {
      "epoch": 1.9184,
      "grad_norm": 0.46347835659980774,
      "learning_rate": 0.0001837987012987013,
      "loss": 2.4012,
      "step": 600
    },
    {
      "epoch": 2.0768,
      "grad_norm": 0.5380644798278809,
      "learning_rate": 0.00018217532467532468,
      "loss": 2.3635,
      "step": 650
    },
    {
      "epoch": 2.2368,
      "grad_norm": 0.5009892582893372,
      "learning_rate": 0.00018055194805194805,
      "loss": 2.3559,
      "step": 700
    },
    {
      "epoch": 2.3968,
      "grad_norm": 0.5202702879905701,
      "learning_rate": 0.00017892857142857144,
      "loss": 2.3629,
      "step": 750
    },
    {
      "epoch": 2.5568,
      "grad_norm": 0.5047163367271423,
      "learning_rate": 0.0001773051948051948,
      "loss": 2.3988,
      "step": 800
    },
    {
      "epoch": 2.7168,
      "grad_norm": 0.5131796002388,
      "learning_rate": 0.00017568181818181818,
      "loss": 2.3572,
      "step": 850
    },
    {
      "epoch": 2.8768000000000002,
      "grad_norm": 0.4842478930950165,
      "learning_rate": 0.00017405844155844158,
      "loss": 2.348,
      "step": 900
    },
    {
      "epoch": 3.0352,
      "grad_norm": 0.5232282280921936,
      "learning_rate": 0.00017243506493506495,
      "loss": 2.2918,
      "step": 950
    },
    {
      "epoch": 3.1952,
      "grad_norm": 0.5291454195976257,
      "learning_rate": 0.00017081168831168831,
      "loss": 2.3136,
      "step": 1000
    },
    {
      "epoch": 3.3552,
      "grad_norm": 0.5820225477218628,
      "learning_rate": 0.0001691883116883117,
      "loss": 2.3009,
      "step": 1050
    },
    {
      "epoch": 3.5152,
      "grad_norm": 0.5244321823120117,
      "learning_rate": 0.00016756493506493508,
      "loss": 2.334,
      "step": 1100
    },
    {
      "epoch": 3.6752000000000002,
      "grad_norm": 0.5090017318725586,
      "learning_rate": 0.00016594155844155845,
      "loss": 2.3094,
      "step": 1150
    },
    {
      "epoch": 3.8352,
      "grad_norm": 0.5572848320007324,
      "learning_rate": 0.00016431818181818182,
      "loss": 2.2926,
      "step": 1200
    },
    {
      "epoch": 3.9952,
      "grad_norm": 0.518208384513855,
      "learning_rate": 0.0001626948051948052,
      "loss": 2.3039,
      "step": 1250
    },
    {
      "epoch": 4.1536,
      "grad_norm": 0.568171501159668,
      "learning_rate": 0.00016107142857142858,
      "loss": 2.2646,
      "step": 1300
    },
    {
      "epoch": 4.3136,
      "grad_norm": 0.5639809370040894,
      "learning_rate": 0.00015944805194805195,
      "loss": 2.2469,
      "step": 1350
    },
    {
      "epoch": 4.4736,
      "grad_norm": 0.549776554107666,
      "learning_rate": 0.00015782467532467534,
      "loss": 2.2334,
      "step": 1400
    },
    {
      "epoch": 4.6336,
      "grad_norm": 0.6122168302536011,
      "learning_rate": 0.0001562012987012987,
      "loss": 2.2726,
      "step": 1450
    },
    {
      "epoch": 4.7936,
      "grad_norm": 0.5819776654243469,
      "learning_rate": 0.00015457792207792208,
      "loss": 2.3203,
      "step": 1500
    },
    {
      "epoch": 4.9536,
      "grad_norm": 0.5856132507324219,
      "learning_rate": 0.00015295454545454545,
      "loss": 2.2691,
      "step": 1550
    },
    {
      "epoch": 5.112,
      "grad_norm": 0.5701330900192261,
      "learning_rate": 0.00015133116883116884,
      "loss": 2.256,
      "step": 1600
    },
    {
      "epoch": 5.272,
      "grad_norm": 0.6040866374969482,
      "learning_rate": 0.0001497077922077922,
      "loss": 2.2096,
      "step": 1650
    },
    {
      "epoch": 5.432,
      "grad_norm": 0.6371007561683655,
      "learning_rate": 0.00014808441558441558,
      "loss": 2.2359,
      "step": 1700
    },
    {
      "epoch": 5.592,
      "grad_norm": 0.6163896918296814,
      "learning_rate": 0.00014646103896103898,
      "loss": 2.25,
      "step": 1750
    },
    {
      "epoch": 5.752,
      "grad_norm": 0.576591432094574,
      "learning_rate": 0.00014483766233766235,
      "loss": 2.242,
      "step": 1800
    },
    {
      "epoch": 5.912,
      "grad_norm": 0.6064857244491577,
      "learning_rate": 0.00014321428571428571,
      "loss": 2.2267,
      "step": 1850
    },
    {
      "epoch": 6.0704,
      "grad_norm": 0.5921481251716614,
      "learning_rate": 0.0001415909090909091,
      "loss": 2.1961,
      "step": 1900
    },
    {
      "epoch": 6.2304,
      "grad_norm": 0.6759140491485596,
      "learning_rate": 0.00013996753246753248,
      "loss": 2.1435,
      "step": 1950
    },
    {
      "epoch": 6.3904,
      "grad_norm": 0.7134625911712646,
      "learning_rate": 0.00013834415584415585,
      "loss": 2.2143,
      "step": 2000
    },
    {
      "epoch": 6.5504,
      "grad_norm": 0.6537560224533081,
      "learning_rate": 0.00013672077922077924,
      "loss": 2.2003,
      "step": 2050
    },
    {
      "epoch": 6.7104,
      "grad_norm": 0.6627336740493774,
      "learning_rate": 0.0001350974025974026,
      "loss": 2.2453,
      "step": 2100
    },
    {
      "epoch": 6.8704,
      "grad_norm": 0.6321182250976562,
      "learning_rate": 0.00013347402597402598,
      "loss": 2.197,
      "step": 2150
    },
    {
      "epoch": 7.0288,
      "grad_norm": 0.6316569447517395,
      "learning_rate": 0.00013185064935064935,
      "loss": 2.1825,
      "step": 2200
    },
    {
      "epoch": 7.1888,
      "grad_norm": 0.6656830310821533,
      "learning_rate": 0.00013022727272727274,
      "loss": 2.1301,
      "step": 2250
    },
    {
      "epoch": 7.3488,
      "grad_norm": 0.7097733616828918,
      "learning_rate": 0.0001286038961038961,
      "loss": 2.1706,
      "step": 2300
    },
    {
      "epoch": 7.5088,
      "grad_norm": 0.7125844359397888,
      "learning_rate": 0.00012698051948051948,
      "loss": 2.1688,
      "step": 2350
    },
    {
      "epoch": 7.6688,
      "grad_norm": 0.7158123850822449,
      "learning_rate": 0.00012535714285714285,
      "loss": 2.1475,
      "step": 2400
    },
    {
      "epoch": 7.8288,
      "grad_norm": 0.6995317339897156,
      "learning_rate": 0.00012373376623376624,
      "loss": 2.1719,
      "step": 2450
    },
    {
      "epoch": 7.9888,
      "grad_norm": 0.717815101146698,
      "learning_rate": 0.0001221103896103896,
      "loss": 2.2235,
      "step": 2500
    },
    {
      "epoch": 8.1472,
      "grad_norm": 0.7461234927177429,
      "learning_rate": 0.000120487012987013,
      "loss": 2.1349,
      "step": 2550
    },
    {
      "epoch": 8.3072,
      "grad_norm": 0.761265218257904,
      "learning_rate": 0.00011886363636363638,
      "loss": 2.1221,
      "step": 2600
    },
    {
      "epoch": 8.4672,
      "grad_norm": 0.7637495398521423,
      "learning_rate": 0.00011724025974025975,
      "loss": 2.1404,
      "step": 2650
    },
    {
      "epoch": 8.6272,
      "grad_norm": 0.6961876749992371,
      "learning_rate": 0.00011561688311688313,
      "loss": 2.1201,
      "step": 2700
    },
    {
      "epoch": 8.7872,
      "grad_norm": 0.7136393189430237,
      "learning_rate": 0.00011399350649350651,
      "loss": 2.1351,
      "step": 2750
    },
    {
      "epoch": 8.9472,
      "grad_norm": 0.7170557975769043,
      "learning_rate": 0.00011237012987012986,
      "loss": 2.16,
      "step": 2800
    },
    {
      "epoch": 9.1056,
      "grad_norm": 0.7341310977935791,
      "learning_rate": 0.00011074675324675326,
      "loss": 2.0828,
      "step": 2850
    },
    {
      "epoch": 9.2656,
      "grad_norm": 0.7363979816436768,
      "learning_rate": 0.00010912337662337664,
      "loss": 2.0695,
      "step": 2900
    },
    {
      "epoch": 9.4256,
      "grad_norm": 0.8130752444267273,
      "learning_rate": 0.0001075,
      "loss": 2.1076,
      "step": 2950
    },
    {
      "epoch": 9.5856,
      "grad_norm": 0.8436978459358215,
      "learning_rate": 0.00010587662337662338,
      "loss": 2.1299,
      "step": 3000
    },
    {
      "epoch": 9.7456,
      "grad_norm": 0.7438020706176758,
      "learning_rate": 0.00010425324675324675,
      "loss": 2.1257,
      "step": 3050
    },
    {
      "epoch": 9.9056,
      "grad_norm": 0.7462236285209656,
      "learning_rate": 0.00010262987012987013,
      "loss": 2.1735,
      "step": 3100
    },
    {
      "epoch": 10.064,
      "grad_norm": 0.7880806922912598,
      "learning_rate": 0.00010100649350649351,
      "loss": 2.0966,
      "step": 3150
    },
    {
      "epoch": 10.224,
      "grad_norm": 0.804535448551178,
      "learning_rate": 9.93831168831169e-05,
      "loss": 2.0809,
      "step": 3200
    },
    {
      "epoch": 10.384,
      "grad_norm": 0.8423541188240051,
      "learning_rate": 9.775974025974026e-05,
      "loss": 2.0873,
      "step": 3250
    },
    {
      "epoch": 10.544,
      "grad_norm": 0.7899349331855774,
      "learning_rate": 9.613636363636363e-05,
      "loss": 2.0864,
      "step": 3300
    },
    {
      "epoch": 10.704,
      "grad_norm": 0.7400680780410767,
      "learning_rate": 9.451298701298703e-05,
      "loss": 2.1104,
      "step": 3350
    },
    {
      "epoch": 10.864,
      "grad_norm": 0.8808563351631165,
      "learning_rate": 9.28896103896104e-05,
      "loss": 2.0945,
      "step": 3400
    },
    {
      "epoch": 11.0224,
      "grad_norm": 0.8270837068557739,
      "learning_rate": 9.126623376623376e-05,
      "loss": 2.0665,
      "step": 3450
    },
    {
      "epoch": 11.1824,
      "grad_norm": 0.804492712020874,
      "learning_rate": 8.964285714285715e-05,
      "loss": 2.0295,
      "step": 3500
    },
    {
      "epoch": 11.3424,
      "grad_norm": 0.8137935996055603,
      "learning_rate": 8.801948051948053e-05,
      "loss": 2.0592,
      "step": 3550
    },
    {
      "epoch": 11.5024,
      "grad_norm": 0.878435492515564,
      "learning_rate": 8.63961038961039e-05,
      "loss": 2.0731,
      "step": 3600
    },
    {
      "epoch": 11.6624,
      "grad_norm": 0.8696560263633728,
      "learning_rate": 8.477272727272728e-05,
      "loss": 2.0897,
      "step": 3650
    },
    {
      "epoch": 11.8224,
      "grad_norm": 0.8247829079627991,
      "learning_rate": 8.314935064935066e-05,
      "loss": 2.0688,
      "step": 3700
    },
    {
      "epoch": 11.9824,
      "grad_norm": 0.9064407348632812,
      "learning_rate": 8.152597402597403e-05,
      "loss": 2.0705,
      "step": 3750
    },
    {
      "epoch": 12.1408,
      "grad_norm": 0.8644624352455139,
      "learning_rate": 7.99025974025974e-05,
      "loss": 2.0732,
      "step": 3800
    },
    {
      "epoch": 12.3008,
      "grad_norm": 0.8577916622161865,
      "learning_rate": 7.827922077922078e-05,
      "loss": 2.0453,
      "step": 3850
    },
    {
      "epoch": 12.4608,
      "grad_norm": 0.8568642735481262,
      "learning_rate": 7.665584415584416e-05,
      "loss": 2.0165,
      "step": 3900
    },
    {
      "epoch": 12.6208,
      "grad_norm": 0.8307271599769592,
      "learning_rate": 7.503246753246753e-05,
      "loss": 2.0705,
      "step": 3950
    },
    {
      "epoch": 12.7808,
      "grad_norm": 0.8952555656433105,
      "learning_rate": 7.340909090909091e-05,
      "loss": 2.0235,
      "step": 4000
    },
    {
      "epoch": 12.9408,
      "grad_norm": 0.8733186721801758,
      "learning_rate": 7.17857142857143e-05,
      "loss": 2.0556,
      "step": 4050
    },
    {
      "epoch": 13.0992,
      "grad_norm": 0.9276560544967651,
      "learning_rate": 7.016233766233766e-05,
      "loss": 1.9921,
      "step": 4100
    },
    {
      "epoch": 13.2592,
      "grad_norm": 0.8835634589195251,
      "learning_rate": 6.853896103896104e-05,
      "loss": 2.0042,
      "step": 4150
    },
    {
      "epoch": 13.4192,
      "grad_norm": 0.9113519787788391,
      "learning_rate": 6.691558441558443e-05,
      "loss": 2.0553,
      "step": 4200
    },
    {
      "epoch": 13.5792,
      "grad_norm": 0.8690248727798462,
      "learning_rate": 6.52922077922078e-05,
      "loss": 2.0514,
      "step": 4250
    },
    {
      "epoch": 13.7392,
      "grad_norm": 0.900356113910675,
      "learning_rate": 6.366883116883116e-05,
      "loss": 2.0335,
      "step": 4300
    },
    {
      "epoch": 13.8992,
      "grad_norm": 0.8367992043495178,
      "learning_rate": 6.204545454545455e-05,
      "loss": 2.0507,
      "step": 4350
    },
    {
      "epoch": 14.0576,
      "grad_norm": 0.970832347869873,
      "learning_rate": 6.042207792207793e-05,
      "loss": 2.0202,
      "step": 4400
    },
    {
      "epoch": 14.2176,
      "grad_norm": 0.8872577548027039,
      "learning_rate": 5.8798701298701296e-05,
      "loss": 1.9717,
      "step": 4450
    },
    {
      "epoch": 14.3776,
      "grad_norm": 0.954136312007904,
      "learning_rate": 5.7175324675324685e-05,
      "loss": 1.9948,
      "step": 4500
    },
    {
      "epoch": 14.5376,
      "grad_norm": 0.8866714239120483,
      "learning_rate": 5.5551948051948053e-05,
      "loss": 2.0061,
      "step": 4550
    },
    {
      "epoch": 14.6976,
      "grad_norm": 0.8986603021621704,
      "learning_rate": 5.392857142857143e-05,
      "loss": 2.0307,
      "step": 4600
    },
    {
      "epoch": 14.8576,
      "grad_norm": 0.9674103260040283,
      "learning_rate": 5.230519480519481e-05,
      "loss": 2.0266,
      "step": 4650
    },
    {
      "epoch": 15.016,
      "grad_norm": 0.8958767652511597,
      "learning_rate": 5.0681818181818186e-05,
      "loss": 2.0417,
      "step": 4700
    },
    {
      "epoch": 15.176,
      "grad_norm": 0.9604573249816895,
      "learning_rate": 4.905844155844156e-05,
      "loss": 1.983,
      "step": 4750
    },
    {
      "epoch": 15.336,
      "grad_norm": 0.8792793154716492,
      "learning_rate": 4.7435064935064937e-05,
      "loss": 2.0144,
      "step": 4800
    },
    {
      "epoch": 15.496,
      "grad_norm": 1.0177017450332642,
      "learning_rate": 4.581168831168832e-05,
      "loss": 1.9627,
      "step": 4850
    },
    {
      "epoch": 15.656,
      "grad_norm": 0.9542194604873657,
      "learning_rate": 4.418831168831169e-05,
      "loss": 2.0177,
      "step": 4900
    },
    {
      "epoch": 15.816,
      "grad_norm": 0.9740577340126038,
      "learning_rate": 4.256493506493507e-05,
      "loss": 1.9905,
      "step": 4950
    },
    {
      "epoch": 15.975999999999999,
      "grad_norm": 0.9841474294662476,
      "learning_rate": 4.0941558441558444e-05,
      "loss": 2.0364,
      "step": 5000
    },
    {
      "epoch": 16.1344,
      "grad_norm": 1.041202187538147,
      "learning_rate": 3.931818181818182e-05,
      "loss": 1.9969,
      "step": 5050
    },
    {
      "epoch": 16.2944,
      "grad_norm": 0.9855248332023621,
      "learning_rate": 3.7694805194805195e-05,
      "loss": 1.9837,
      "step": 5100
    },
    {
      "epoch": 16.4544,
      "grad_norm": 1.0262808799743652,
      "learning_rate": 3.607142857142857e-05,
      "loss": 1.99,
      "step": 5150
    },
    {
      "epoch": 16.6144,
      "grad_norm": 1.0004459619522095,
      "learning_rate": 3.444805194805195e-05,
      "loss": 1.9769,
      "step": 5200
    },
    {
      "epoch": 16.7744,
      "grad_norm": 0.9941471815109253,
      "learning_rate": 3.282467532467533e-05,
      "loss": 1.9823,
      "step": 5250
    },
    {
      "epoch": 16.9344,
      "grad_norm": 1.0051857233047485,
      "learning_rate": 3.12012987012987e-05,
      "loss": 1.9918,
      "step": 5300
    },
    {
      "epoch": 17.0928,
      "grad_norm": 0.9701197147369385,
      "learning_rate": 2.9577922077922078e-05,
      "loss": 1.9633,
      "step": 5350
    },
    {
      "epoch": 17.2528,
      "grad_norm": 0.9354686737060547,
      "learning_rate": 2.7954545454545457e-05,
      "loss": 1.9506,
      "step": 5400
    },
    {
      "epoch": 17.4128,
      "grad_norm": 1.0257920026779175,
      "learning_rate": 2.6331168831168835e-05,
      "loss": 1.9646,
      "step": 5450
    },
    {
      "epoch": 17.5728,
      "grad_norm": 0.9540081024169922,
      "learning_rate": 2.470779220779221e-05,
      "loss": 1.994,
      "step": 5500
    },
    {
      "epoch": 17.7328,
      "grad_norm": 1.0210108757019043,
      "learning_rate": 2.3084415584415586e-05,
      "loss": 1.9971,
      "step": 5550
    },
    {
      "epoch": 17.8928,
      "grad_norm": 0.9642616510391235,
      "learning_rate": 2.146103896103896e-05,
      "loss": 1.9865,
      "step": 5600
    },
    {
      "epoch": 18.0512,
      "grad_norm": 0.9718641638755798,
      "learning_rate": 1.983766233766234e-05,
      "loss": 1.9699,
      "step": 5650
    },
    {
      "epoch": 18.2112,
      "grad_norm": 1.0174692869186401,
      "learning_rate": 1.8214285714285715e-05,
      "loss": 1.9466,
      "step": 5700
    },
    {
      "epoch": 18.3712,
      "grad_norm": 1.0433961153030396,
      "learning_rate": 1.6590909090909094e-05,
      "loss": 1.9573,
      "step": 5750
    },
    {
      "epoch": 18.5312,
      "grad_norm": 0.9960147142410278,
      "learning_rate": 1.4967532467532469e-05,
      "loss": 1.9857,
      "step": 5800
    },
    {
      "epoch": 18.6912,
      "grad_norm": 1.0107501745224,
      "learning_rate": 1.3344155844155844e-05,
      "loss": 1.9798,
      "step": 5850
    },
    {
      "epoch": 18.8512,
      "grad_norm": 1.0575969219207764,
      "learning_rate": 1.1720779220779221e-05,
      "loss": 1.9693,
      "step": 5900
    },
    {
      "epoch": 19.0096,
      "grad_norm": 0.9875307083129883,
      "learning_rate": 1.0097402597402598e-05,
      "loss": 1.9707,
      "step": 5950
    },
    {
      "epoch": 19.1696,
      "grad_norm": 1.0090218782424927,
      "learning_rate": 8.474025974025975e-06,
      "loss": 1.9618,
      "step": 6000
    },
    {
      "epoch": 19.3296,
      "grad_norm": 1.0202420949935913,
      "learning_rate": 6.8506493506493505e-06,
      "loss": 1.9689,
      "step": 6050
    },
    {
      "epoch": 19.4896,
      "grad_norm": 0.9694007039070129,
      "learning_rate": 5.2272727272727274e-06,
      "loss": 1.9562,
      "step": 6100
    },
    {
      "epoch": 19.6496,
      "grad_norm": 0.995557963848114,
      "learning_rate": 3.603896103896104e-06,
      "loss": 1.961,
      "step": 6150
    },
    {
      "epoch": 19.8096,
      "grad_norm": 1.0142896175384521,
      "learning_rate": 1.9805194805194805e-06,
      "loss": 1.9398,
      "step": 6200
    },
    {
      "epoch": 19.9696,
      "grad_norm": 0.9631243944168091,
      "learning_rate": 3.5714285714285716e-07,
      "loss": 1.9854,
      "step": 6250
    }
  ],
  "logging_steps": 50,
  "max_steps": 6260,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.999973838848e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
